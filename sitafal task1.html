import os
import numpy as np
from PyPDF2 import PdfReader
from sklearn.neighbors import NearestNeighbors
from tqdm import tqdm
from sentence_transformers import SentenceTransformer


pdf_dir = r"C:\Users\vanda\Downloads\Task 1"  
if not os.path.exists(pdf_dir):
    raise FileNotFoundError(f"The directory '{pdf_dir}' does not exist. Please provide a valid directory.")
    
model = SentenceTransformer('all-MiniLM-L6-v2')


def extract_text_from_pdf(pdf_path, pages=[2, 6]):
    text_chunks = []
    reader = PdfReader(pdf_path)
    
   
    total_pages = len(reader.pages)
    for page_num in pages:
        if page_num <= total_pages:
            text = reader.pages[page_num - 1].extract_text()  
            if text:
                text_chunks.append(text)
    return text_chunks

def chunk_text(text, max_chunk_size=500):
    words = text.split()
    chunks = []
    current_chunk = []
    for word in words:
        current_chunk.append(word)
        if len(current_chunk) >= max_chunk_size:
            chunks.append(" ".join(current_chunk))
            current_chunk = []
    if current_chunk:
        chunks.append(" ".join(current_chunk))
    return chunks


def store_embeddings_in_knn(embeddings, metadata):
    knn = NearestNeighbors(n_neighbors=min(5, len(embeddings)), algorithm='auto', metric='cosine')
    knn.fit(embeddings)
    return knn, metadata


all_embeddings = []
metadata = []
for pdf_file in tqdm(os.listdir(pdf_dir)):
    if pdf_file.endswith(".pdf"):
        pdf_path = os.path.join(pdf_dir, pdf_file)
        text_chunks = extract_text_from_pdf(pdf_path)
        for text in text_chunks:
            chunks = chunk_text(text)
            embeddings = model.encode(chunks)  
            all_embeddings.extend(embeddings)
            metadata.extend([(pdf_file, chunk) for chunk in chunks])


print(f"Total embeddings generated: {len(all_embeddings)}")

if len(all_embeddings) == 0:
    raise ValueError("No embeddings were generated. Please check the PDF content and extraction process.")


all_embeddings = np.array(all_embeddings)


knn, metadata = store_embeddings_in_knn(all_embeddings, metadata)


def query_knn(query, knn, metadata, top_k=5):
    query_embedding = model.encode([query]).reshape(1, -1)
    
    
    n_neighbors = min(top_k, len(all_embeddings))
    
    distances, indices = knn.kneighbors(query_embedding, n_neighbors=n_neighbors)
    results = [(metadata[idx], distances[0][i]) for i, idx in enumerate(indices[0])]
    return results


query = "What is the unemployment rate for people with a bachelor's degree?"
results = query_knn(query, knn, metadata)
print(query)

for (file_chunk, score) in results:
    print(f"File: {file_chunk[0]}, Chunk: {file_chunk[1]}, Score: {score}")
print()


query = "Extract the tabular data on economic indicators."
results = query_knn(query, knn, metadata)
print(query)

for (file_chunk, score) in results:
    print(f"File: {file_chunk[0]}, Chunk: {file_chunk[1]}, Score: {score}")
print()


query = "Compare the unemployment rate for people with bachelor's and master's degrees."
results = query_knn(query, knn, metadata)
print(query)

for (file_chunk, score) in results:
    print(f"File: {file_chunk[0]}, Chunk: {file_chunk[1]}, Score: {score}")
    print(query)